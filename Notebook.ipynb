{
 "cells": [
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from sklearn.utils import shuffle\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.regularizers import l1, l2"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n",
     "name": "stderr"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class Data:\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = (112, 112, 3)\n",
    "\n",
    "    def loadTrainData(self, trainFile, trainLabelsFile):\n",
    "        trainData = np.load(trainFile)\n",
    "        trainData = trainData.reshape(trainData.shape[0], trainData.shape[2], trainData.shape[3], trainData.shape[1])\n",
    "\n",
    "        trainLabels = np.load(trainLabelsFile)\n",
    "        trainLabels = keras.utils.to_categorical(trainLabels)\n",
    "\n",
    "        trainData, trainLabels = shuffle(trainData, trainLabels)\n",
    "        \n",
    "        validationSize = 12000\n",
    "        validationData = trainData[validationSize:]\n",
    "        validationLabels = trainLabels[validationSize:]\n",
    "\n",
    "        trainData = trainData[:validationSize]\n",
    "        trainLabels = trainLabels[:validationSize]\n",
    "\n",
    "        return trainData, trainLabels, validationData, validationLabels\n",
    "\n",
    "    def loadTestData(self, testFile, testLabelsFile):\n",
    "        testData = np.load(testFile)\n",
    "        testData = testData.reshape(testData.shape[0], testData.shape[2], testData.shape[3], testData.shape[1])\n",
    "\n",
    "        testLabels = np.load(testLabelsFile)\n",
    "        testLabels = keras.utils.to_categorical(testLabels)\n",
    "\n",
    "        return testData, testLabels\n",
    "\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "class Model:\n",
    "    def __init__(self, input_dim, classes, lr=0.001, epochs=1):\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = classes\n",
    "        self.learning_rate = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def lr_schedule(self, epoch):\n",
    "        if epoch % 10 == 0:\n",
    "            self.learning_rate = self.learning_rate/10\n",
    "        print('Learning rate: ', self.learning_rate)\n",
    "        return self.learning_rate\n",
    "\n",
    "    def model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal', activation='relu', input_shape=self.input_dim))\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal', activation='relu'))\n",
    "        model.add(BatchNormalization(momentum=0.9, epsilon=0.0001))\n",
    "        \n",
    "        model.add(AveragePooling2D(pool_size=(3, 3)))\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=(5, 5), padding='same', activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Conv2D(512, kernel_size=(5, 5), padding='same', activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(BatchNormalization(momentum=0.9, epsilon=0.0001))\n",
    "        \n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Dense(512, kernel_initializer='he_normal', activation='relu'))   \n",
    "        model.add(Dense(128, kernel_initializer='he_normal', activation='relu'))            \n",
    "        model.add(Dense(64, kernel_initializer='he_normal', activation='relu'))\n",
    "        model.add(Dense(32, kernel_initializer='he_normal', activation='relu'))\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=Adam(lr=self.lr_schedule(1)), #lr=self.learning_rate, decay=self.learning_rate/self.epochs),\n",
    "                      loss=keras.losses.categorical_crossentropy,\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "trainDataFile = \"/kaggle/input/minidata/train.npy\"  # sys.argv[1]\ntrainLabelsFile = \"/kaggle/input/minidata/trainlabels.npy\"  # sys.argv[2]\nmodelFile = \"model64.h5\"  # sys.argv[3]\n\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-2\nEPOCHS = 50\nNUM_CLASSES = 10\n\ndataObj = Data(BATCH_SIZE)\ntrainData, trainLabels, valData, valLabels = dataObj.loadTrainData(trainDataFile, trainLabelsFile)\n\ntrainDataMean = np.mean(trainData, axis=0)\ntrainData -= trainDataMean\n\nvalDataMean = np.mean(valData, axis=0)\nvalData -= valDataMean\n\nprint(\"Train Data\", trainData.shape)\nprint(\"Train Labels\", trainLabels.shape)\n\nprint(\"Validation Data\", valData.shape)\nprint(\"Validation Labels\", valLabels.shape)\nprint(valLabels[0], trainLabels[0])\n\nmodel = Model(trainData[0].shape, NUM_CLASSES, LEARNING_RATE, EPOCHS)\n\nlrScheduler = LearningRateScheduler(model.lr_schedule)\nearlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nmodelCheckpoint = ModelCheckpoint(modelFile, monitor='val_loss', verbose=1, save_best_only=True)\n\nmodel = model.model()\n#model.load_weights(modelFile)\n\nmodel.fit(\n    trainData, trainLabels,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    validation_data=(valData, valLabels),\n    epochs=EPOCHS,\n    callbacks=[earlyStopping, lrScheduler, modelCheckpoint],\n    verbose=1\n)",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Train Data (12000, 112, 112, 3)\nTrain Labels (12000, 10)\nValidation Data (730, 112, 112, 3)\nValidation Labels (730, 10)\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\nLearning rate:  0.01\nTrain on 12000 samples, validate on 730 samples\nEpoch 1/50\nLearning rate:  0.001\n12000/12000 [==============================] - 45s 4ms/step - loss: 1.8233 - accuracy: 0.3323 - val_loss: 1.9412 - val_accuracy: 0.3055\n\nEpoch 00001: val_loss improved from inf to 1.94123, saving model to model64.h5\nEpoch 2/50\nLearning rate:  0.001\n12000/12000 [==============================] - 39s 3ms/step - loss: 1.5176 - accuracy: 0.4571 - val_loss: 1.9509 - val_accuracy: 0.3479\n\nEpoch 00002: val_loss did not improve from 1.94123\nEpoch 3/50\nLearning rate:  0.001\n12000/12000 [==============================] - 39s 3ms/step - loss: 1.3293 - accuracy: 0.5375 - val_loss: 2.2562 - val_accuracy: 0.3986\n\nEpoch 00003: val_loss did not improve from 1.94123\nEpoch 4/50\nLearning rate:  0.001\n12000/12000 [==============================] - 39s 3ms/step - loss: 1.1863 - accuracy: 0.5849 - val_loss: 1.3302 - val_accuracy: 0.5397\n\nEpoch 00004: val_loss improved from 1.94123 to 1.33022, saving model to model64.h5\nEpoch 5/50\nLearning rate:  0.001\n12000/12000 [==============================] - 39s 3ms/step - loss: 1.0876 - accuracy: 0.6311 - val_loss: 1.0577 - val_accuracy: 0.6479\n\nEpoch 00005: val_loss improved from 1.33022 to 1.05767, saving model to model64.h5\nEpoch 6/50\nLearning rate:  0.001\n12000/12000 [==============================] - 39s 3ms/step - loss: 1.0159 - accuracy: 0.6537 - val_loss: 1.3228 - val_accuracy: 0.5877\n\nEpoch 00006: val_loss did not improve from 1.05767\nEpoch 7/50\nLearning rate:  0.001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.9632 - accuracy: 0.6740 - val_loss: 1.0778 - val_accuracy: 0.6178\n\nEpoch 00007: val_loss did not improve from 1.05767\nEpoch 8/50\nLearning rate:  0.001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.8792 - accuracy: 0.7007 - val_loss: 1.2260 - val_accuracy: 0.6452\n\nEpoch 00008: val_loss did not improve from 1.05767\nEpoch 9/50\nLearning rate:  0.001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.8327 - accuracy: 0.7156 - val_loss: 0.8648 - val_accuracy: 0.7178\n\nEpoch 00009: val_loss improved from 1.05767 to 0.86477, saving model to model64.h5\nEpoch 10/50\nLearning rate:  0.001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.7858 - accuracy: 0.7373 - val_loss: 0.9320 - val_accuracy: 0.7014\n\nEpoch 00010: val_loss did not improve from 0.86477\nEpoch 11/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.6332 - accuracy: 0.7848 - val_loss: 0.7167 - val_accuracy: 0.7753\n\nEpoch 00011: val_loss improved from 0.86477 to 0.71674, saving model to model64.h5\nEpoch 12/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.5853 - accuracy: 0.8016 - val_loss: 0.7076 - val_accuracy: 0.7781\n\nEpoch 00012: val_loss improved from 0.71674 to 0.70762, saving model to model64.h5\nEpoch 13/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.5663 - accuracy: 0.8054 - val_loss: 0.7063 - val_accuracy: 0.7849\n\nEpoch 00013: val_loss improved from 0.70762 to 0.70634, saving model to model64.h5\nEpoch 14/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.5521 - accuracy: 0.8108 - val_loss: 0.7149 - val_accuracy: 0.7849\n\nEpoch 00014: val_loss did not improve from 0.70634\nEpoch 15/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.5387 - accuracy: 0.8115 - val_loss: 0.7045 - val_accuracy: 0.7877\n\nEpoch 00015: val_loss improved from 0.70634 to 0.70449, saving model to model64.h5\nEpoch 16/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.5312 - accuracy: 0.8175 - val_loss: 0.7037 - val_accuracy: 0.7877\n\nEpoch 00016: val_loss improved from 0.70449 to 0.70368, saving model to model64.h5\nEpoch 17/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.5164 - accuracy: 0.8259 - val_loss: 0.6907 - val_accuracy: 0.7890\n\nEpoch 00017: val_loss improved from 0.70368 to 0.69070, saving model to model64.h5\nEpoch 18/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.5018 - accuracy: 0.8304 - val_loss: 0.6852 - val_accuracy: 0.7986\n\nEpoch 00018: val_loss improved from 0.69070 to 0.68523, saving model to model64.h5\nEpoch 19/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4908 - accuracy: 0.8336 - val_loss: 0.6911 - val_accuracy: 0.7973\n\nEpoch 00019: val_loss did not improve from 0.68523\nEpoch 20/50\nLearning rate:  0.0001\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4901 - accuracy: 0.8303 - val_loss: 0.6781 - val_accuracy: 0.7945\n\nEpoch 00020: val_loss improved from 0.68523 to 0.67814, saving model to model64.h5\nEpoch 21/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4593 - accuracy: 0.8367 - val_loss: 0.6735 - val_accuracy: 0.7973\n\nEpoch 00021: val_loss improved from 0.67814 to 0.67351, saving model to model64.h5\nEpoch 22/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4508 - accuracy: 0.8470 - val_loss: 0.6816 - val_accuracy: 0.7945\n\nEpoch 00022: val_loss did not improve from 0.67351\nEpoch 23/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4541 - accuracy: 0.8403 - val_loss: 0.6836 - val_accuracy: 0.7973\n\nEpoch 00023: val_loss did not improve from 0.67351\nEpoch 24/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4556 - accuracy: 0.8430 - val_loss: 0.6776 - val_accuracy: 0.8041\n\nEpoch 00024: val_loss did not improve from 0.67351\nEpoch 25/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4476 - accuracy: 0.8460 - val_loss: 0.6884 - val_accuracy: 0.8014\n\nEpoch 00025: val_loss did not improve from 0.67351\nEpoch 26/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4488 - accuracy: 0.8457 - val_loss: 0.6858 - val_accuracy: 0.8068\n\nEpoch 00026: val_loss did not improve from 0.67351\nEpoch 27/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4460 - accuracy: 0.8448 - val_loss: 0.6850 - val_accuracy: 0.8027\n\nEpoch 00027: val_loss did not improve from 0.67351\nEpoch 28/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4454 - accuracy: 0.8458 - val_loss: 0.6871 - val_accuracy: 0.8014\n\nEpoch 00028: val_loss did not improve from 0.67351\nEpoch 29/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4442 - accuracy: 0.8442 - val_loss: 0.6942 - val_accuracy: 0.7986\n\nEpoch 00029: val_loss did not improve from 0.67351\nEpoch 30/50\nLearning rate:  1e-05\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4397 - accuracy: 0.8490 - val_loss: 0.7011 - val_accuracy: 0.7986\n\nEpoch 00030: val_loss did not improve from 0.67351\nEpoch 31/50\nLearning rate:  1.0000000000000002e-06\n12000/12000 [==============================] - 39s 3ms/step - loss: 0.4330 - accuracy: 0.8508 - val_loss: 0.6905 - val_accuracy: 0.8041\n\nEpoch 00031: val_loss did not improve from 0.67351\nEpoch 00031: early stopping\n",
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "execution_count": 4,
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7fda600567f0>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "testDataFile = \"/kaggle/input/minidata/test.npy\" # sys.argv[1] \ntestLabelsFile = \"/kaggle/input/minidata/testlabels.npy\" # sys.argv[2] \nmodelFile = \"model64.h5\" # sys.argv[3]\n\nBATCH_SIZE = 1 \nNUM_CLASSES = 10\n\ndataObj = Data(BATCH_SIZE) \ntestData, testLabels = dataObj.loadTestData(testDataFile, testLabelsFile)\n\ntestDataMean = np.mean(testData, axis=0) \ntestData -= testDataMean\n\nprint(\"Test Data\", testData.shape) \nprint(\"Test Labels\", testLabels.shape)\n\nmodel = load_model(modelFile) \nresults = model.predict(testData)\n\ncount = 0 \nfor i, result in enumerate(results): \n    pred = np.argmax(result) \n    true = np.argmax(testLabels[i])\n    # print(pred, true)\n    if pred == true:\n        count += 1\n        \nprint(\"Accuracy- \", count/len(testData) * 100) \nprint(\"Error- \", (1- (count/len(testData)))* 100)",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Test Data (500, 112, 112, 3)\nTest Labels (500, 10)\nAccuracy-  73.4\nError-  26.6\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}